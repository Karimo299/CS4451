\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{url}  
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Brain Tumor Classification from MRI Radiomic and Image Features\\}


\author{\IEEEauthorblockN{Otilia Pasculescu}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Western University}\\
London, Ontario \\
opascule@uwo.ca}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Christopher Betancur}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Western University}\\
London, Ontario \\
cbetancu@uwo.ca}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle


\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}


\section{Background \& Related Work}

For supervised classification studies for Brain MRI slices, it is common to use publically defined datasets and in this paper we use the Kaggle Brain Tumor dataset. This dataset provides 2D MRI slices with corresponding labels and precomputed first- and second-order radiomic features~\cite{bohajuBrainTumor}.
 

Most of the recent literature on the topic of brain tumour classification from MRI scans utilizes deep convolutional neural networks (CNNs) applied directly to 2D slices.
Abiwinanda et al.\ train a CNN on grayscale brain MRIs that are resized to small
spatial resolutions (e.g., $64\times64$), which shows that even smaller representations of the image, produce good performance~\cite{abiwinanda2019}. Ait~Amou et al.\ propose a deep-learning MRI diagnosis pipeline for brain tumours and explicitly adopt a similar resizing strategy, citing
Abiwinanda et al.\ as motivation~\cite{aitamou2022}. Ait~Amou et al.\ introduces a CNN-based MRI diagnosis method for brain tumor classification where it too uses resized images to feed the CNN, citing Abiwinanda et al.\ as motivation~\cite{aitamou2022}. These papers demonstrate that resized MRI slices can be used to achieve a strong performance while reducing computational cost, but note that they focus only on image-only inputs and do not examine any extra precomputed features based on the MRI scan.

Along with resizing, there are other preprocessing techniques and methods that are used in MRI-based studies. Shinohara et al.\ argue that intensity across many MRI scanners and acquisition protocols may vary in intensity and are never consisentent and due to that normalization is an important step to reduce variation before modeling~\cite{shinohara2014}.
This supports our use of converting the images to grayscale, resizing, and intensity
normalization before feeding the compressed image into the model, alongside standardization of tabular features.

Cortes and Vapnik introduced SVMs as margin-based classifiers that can learn
non-linear decision boundaries via kernel functions such as the radial basis
function (RBF) kernel~\cite{cortes1995support}. 

To apply our models, both SVMs use scikit-learn library which provides standard implementations of classifiers, feature scaling, and cross-validation procedures~\cite{pedregosa2011scikit}.






\section{Methods}

\subsection{Research Objectives}


In this paper we test the following hypothesis:
\begin{quote}
Across the feature subsets we consider, models that combine
image-based MRI features with tabular features achieve higher 
performance in classifying tumour vs non-tumour MRI scans than models
that use a single feature type (tabular-only or image-only), and
for any given feature subset, an RBF-kernel support vector machine (SVM)
performs better the logistic regression model.
\end{quote}

To investigate this hypothesis, we define the following objectives:

\textbf{O1:} Construct seven datasets from the Kaggle Brain Tumor data~\cite{bohajuBrainTumor}
representing different feature subsets: tabular-only (all 13 features, first-order only,
second-order only), image-only features, image + first-order features,
image + second-order features, and image + all tabular features.

\textbf{O2:} Train and tune logistic regression and RBF-kernel SVM classifiers on
each subset using scikit-learn pipelines with standardization, splitting the
data into training and test sets and optimizing hyperparameters via
cross-validation with grid search.

\textbf{O3:} Compare classification performance across feature subsets and models
using held-out test accuracy and macro-averaged F1-score, and analyze which
feature subset and which model (logistic vs SVM) perform best.


\subsection{Research Methodology}

\subsubsection{Dataset and Preprocessing}

We use the Brain Tumor dataset from Kaggle~\cite{bohajuBrainTumor}, which provides
a CSV file with the precomputed radiomic features and corresponding MRI slice
filenames that those features belong to, along with the raw brain MRI images themselves. 

The raw CSV contains $N = 3762$ samples. We check for and drop any rows with
features taking infinite values, but in this dataset no rows are removed. All feature vectors in the dataset have one to one correspondence to a 2D MRI slice along with a binary label \texttt{Class} where label 1 means that the MRI slice contains a tumor and 0 means the MRI slice does not contain a tumor. For each row in the CSV, it includes an \texttt{Image} identifier (to link row of features to image), the class (tumor or no tumor), and 13 tabular features where the features themselves are divided into two types: 
\paragraph{First-order features.}
\begin{itemize}
    \item Mean
    \item Variance
    \item Standard deviation
    \item Skewness
    \item Kurtosis
\end{itemize}

\paragraph{Second-order (texture) features.}
\begin{itemize}
    \item Contrast
    \item Energy
    \item ASM (angular second moment)
    \item Entropy
    \item Homogeneity
    \item Dissimilarity
    \item Correlation
    \item Coarseness
\end{itemize}


So we now construct seven aligned datasets from the same $N$ samples:

\begin{enumerate}
    \item \textbf{D1: First-order only (tabular)} \\
    Utilize the 5 first-order intensity statistics:
    Mean, Variance, Standard Deviation, Skewness, Kurtosis.

    \item \textbf{D2: Second-order only (tabular)} \\
    Utilize the 8 second-order texture features:
    Contrast, Energy, ASM, Entropy, Homogeneity, Dissimilarity, Correlation, Coarseness.

    \item \textbf{D3: All tabular features} \\
    Utilize all 13 tabular features (first-order + second-order).

    \item \textbf{D4: Image only} \\
    Utilize only the grayscale MRI slice, resized to $64\times64$ and normalized.

    \item \textbf{D5: Image + first-order} \\
    Concatenate image pixels with the 5 first-order features.

    \item \textbf{D6: Image + second-order} \\
    Concatenate image pixels with the 8 second-order features.

    \item \textbf{D7: Image + all tabular features} \\
    Concatenate image pixels with all 13 tabular features.
\end{enumerate}

Each MRI slice is loaded in grayscale, resized to a resolution of $64\times64$,
and then normalized to the range $[0, 1]$. By reducing the size of the original MRI slice from $240\times 240$ to $64\times 64$ we reduce the dimensionality (the number of features we pass into the model) while still retaining the overall structural information needed for tumor classification, following prior work that successfully used small resized MRIs for CNN-based brain tumor classification~\cite{abiwinanda2019, aitamou2022}. In MRI analysis, it is common to normalize the intensities so that MRI machine settings have comparable brightness and contrast before we fit any models~\cite{shinohara2014}.

Let $\mathbf{x}_{\text{img}} \in \mathbb{R}^{4096}$ be the flattened representation
(of length $64\times64$) of the compressed and normalized grayscale MRI slice in pixels and $\mathbf{x}_{\text{tab}} \in \mathbb{R}^{13}$ represent the full tabular feature vector for a given MRI slice. 

For each dataset variant defined above, we construct a single feature vector $\mathbf{x}$ from
these components as follows:
\begin{itemize}
    \item tabular-only variants (D1--D3): $\mathbf{x} \subseteq \mathbf{x}_{\text{tab}}$ where $\mathbf{x}$ can contain all tabular features or a subset.
    \item image-only variant (D4): $\mathbf{x} = \mathbf{x}_{\text{img}}$.
    \item Combined variants (D5--D7):
          \[
              \mathbf{x} =
              \begin{bmatrix}
                  \mathbf{x}_{\text{img}} \\
                  \mathbf{x}'
              \end{bmatrix},
          \]
          where $\mathbf{x}'$ as a subvector of
      $\mathbf{x}_{\text{tab}}$ containing either the 5 first-order features,
      the 8 second-order features, or all 13 tabular features.
\end{itemize}

The goal is to learn a function (model) that, given a feature vector $\mathbf{x}$,
outputs a binary label (a standard supervised binary classification task):
\[
  f : \mathcal{X} \to \{0, 1\},
\]
where $\mathcal{X}$ denotes the space of all feature vectors constructed as above.




\subsubsection{Logistic Regression Model}

\subsubsection{Support Vector Machine Model}

SVMs are margin-based classifiers that finds the decision boundary which maximizes the distance between classes ~\cite{cortes1995support}. This model can represent non-linear decision boundaries by implicitly mapping the input features into a higher-dimensional
space through the kernel function. In this paper, we utilize and SVM with a radial basis function (RBF) kernel~\cite{cortes1995support} as our main model,
implemented using the machine learning library scikit-learn~\cite{pedregosa2011scikit}.

For each of the seven datasets D1--D7 we have defined above each with a unique feature configuration, we then train a separate RBF SVM model on the corresponding feature vectors $\mathbf{x}$. To avoid features with large values dominating the kernel, we standardize
all input features to zero mean and unit variance using a \texttt{StandardScaler} inside a scikit-learn \texttt{Pipeline}.

The RBF SVM has two key hyperparameters:
\begin{enumerate}
    \item Regularization parameter $C$ which controls the trade-off between having a wider margin with more training errors versus fitting the training data more strictly with fewer margin violations.
    \item Kernel width parameter $\gamma$ controls the influence of a single training point.
\end{enumerate}



\subsubsection{Training Procedure and Experimental Design}

For each dataset variant D1--D7, we follow the same training and evaluation
protocol for both logistic regression and SVM models. We split each dataset
into 80\% training and 20\% testing using
\texttt{train\_test\_split(..., test\_size=0.2, stratify=y, random\_state=42)}
to preserve the tumour / non-tumour class proportions and ensure
reproducibility.

To stabilise optimisation and make features comparable in scale, we implement
each classifier as a scikit-learn pipeline~\cite{pedregosa2011scikit}. For
logistic regression, the pipeline uses a median imputer, a
\texttt{StandardScaler}, and a \texttt{LogisticRegression} model with
$\ell_1$ or $\ell_2$ regularisation. For the SVM, the pipeline uses a
\texttt{StandardScaler} and an RBF-kernel \texttt{SVC} classifier. In both
models, the scaler operates on the full feature vector $\mathbf{x}$
(tabular features and, when present, flattened image pixels), so that each
feature dimension has approximately zero mean and unit variance.

Hyperparameters are tuned on the training set using stratified
$k$-fold cross-validation with $k=3$, shuffling, and a fixed
\texttt{random\_state}. For logistic regression, the grid search explores
regularisation strengths $C$, penalties ($\ell_1$ vs.\ $\ell_2$), and
compatible solvers (\texttt{liblinear}, \texttt{lbfgs}, \texttt{saga}). For
the SVM, the grid search varies the regularisation parameter $C$ and the RBF
kernel width $\gamma$ over a small logarithmic grid, with the kernel type
fixed to RBF.

In both models, we select the hyperparameter combination that maximises mean
cross-validation accuracy on the training data, retrain the best model on the
full training set, and evaluate it once on the held-out test set. For every
model (logistic vs.\ SVM) and dataset D1--D7, we report test accuracy and
macro-averaged F1-score to compare performance across feature subsets and
classifiers.


\subsubsection{Evaluation Metrics}
For a given model (logistic regression or RBF SVM), we train and tune
hyperparameters and then evaluate performance using three quantities:

\begin{enumerate}
    \item Mean cross-validated accuracy on the training split
          (used only for hyperparameter selection),
    \item Overall accuracy on the held-out test set,
    \item Macro-averaged F1-score on the held-out test set.
\end{enumerate}

To select hyperparameters, we use mean cross-validated accuracy: for each
hyperparameter configuration we run $k$-fold cross-validation on the training
data and compute the average accuracy over the $k$ train/validation splits.
This provides a more stable estimate of generalisation than a single
train/validation split and helps reduce overfitting of hyperparameters.

After choosing the best hyperparameters, we retrain the model on the full
training split and evaluate it once on the held-out test set. On this test
set we report overall accuracy and macro-averaged F1-score to compare how
well each model performs across the different datasets.

We evaluate test performance by comparing the predicted label $\hat{y}$ with
the true label $y$ for every MRI slice. We treat the tumour class as positive
($y=1$) and the no-tumour class as negative ($y=0$), and define:

\begin{itemize}
    \item $\mathrm{TP}$ (true positives): slices where $y=1$ and $\hat{y}=1$
          (tumour correctly predicted),
    \item $\mathrm{TN}$ (true negatives): slices where $y=0$ and $\hat{y}=0$
          (no tumour correctly predicted),
    \item $\mathrm{FP}$ (false positives): slices where $y=0$ but $\hat{y}=1$
          (model predicts a tumour where there is none),
    \item $\mathrm{FN}$ (false negatives): slices where $y=1$ but $\hat{y}=0$
          (model misses a true tumour).
\end{itemize}

The overall \emph{accuracy} is the fraction of correct predictions:
\[
\text{Accuracy}
= \frac{\mathrm{TP} + \mathrm{TN}}
       {\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}.
\]

For each class $c \in \{\text{tumour}, \text{no-tumour}\}$ we compute
precision, recall, and F1-score. For the positive (tumour) class these are
\[
\text{Precision}_+ =
\frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}},
\qquad
\text{Recall}_+ =
\frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},
\]
\[
\text{F1}_+ = 2 \cdot
\frac{\text{Precision}_+ \cdot \text{Recall}_+}
     {\text{Precision}_+ + \text{Recall}_+}.
\]
Analogous quantities $\text{Precision}_-$, $\text{Recall}_-$, and
$\text{F1}_-$ are defined for the no-tumour class. The macro-averaged
F1-score reported in our experiments corresponds to scikit-learn’s
\texttt{f1\_score(y\_test, y\_pred, average="macro")}, which simply computes
\[
\text{F1}_{\text{macro}} = \tfrac{1}{2} \big( \text{F1}_+ + \text{F1}_- \big),
\]
the unweighted mean of the two class-wise F1-scores.


\subsubsection{Threats to Validity and Rationale}

Our study has several limitations that affect how broadly the results can be interpreted.  
First, all experiments use only a single Kaggle dataset~\cite{bohajuBrainTumor} with precomputed radiomic features and 2D slices. How that data was computed and what was used are not fully documented, so performance estimations in this paper may not be able generalize to other scanners, institutions, or clinical populations.

Second, we work at the slice level and do not have patient identifiers. If multiple
MRI slices come from the same patient appear in both the training and test splits, this may
inflate reported accuracy and F1-scores because the model effectively sees very
similar images during training and testing. Since, we there is no provided documentation to verify whether a given MRI slice has a tumor or not, we simply trust that the provided labels are true.

Third, our models are restricted to logistic regression and RBF-kernel SVMs with
relatively small hyperparameter grids. More complex architectures (e.g., CNNs) or
richer hyperparameter searches could change the absolute performance and possibly the
relative ranking of feature configurations. In addition, resizing MRIs to $64\times64$
and flattening them into vectors may discard fine-grained spatial information.

Despite these limitations, our design is appropriate for O1–O3 because it lets us compare
the various feature subsets we have defined above and compare logistic regression versus SVM models. We are able to apply an 80/20 stratified split with cross-validated model selection, and two classifiers provide a clear test of our hypothesis, although the resulting metrics should not be viewed as definitive clinical performance.

\section{Results}

\section{Conclusions \& Future Work}


\begin{thebibliography}{00}

\bibitem{bohajuBrainTumor}
J.~Bohaju, ``Brain Tumor,'' Kaggle, Jul. 2020, ver.~3. [Online]. Available:
\url{https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor}.
doi: 10.34740/KAGGLE/DSV/955413. [Accessed: Dec. 6, 2025].

\bibitem{abiwinanda2019}
N.~Abiwinanda, M.~Hanif, S.~T.~Hesaputra, A.~Handayani, and T.~R.~Mengko,
``Brain tumor classification using convolutional neural network,''
in \emph{World Congress on Medical Physics and Biomedical Engineering 2018}.
Singapore: Springer, 2019, pp.~183--189.
doi: 10.1007/978-981-10-9035-6\_33.

\bibitem{aitamou2022}
M.~Ait~Amou, H.~Xia, A.~Larhmam, and M.~Alami~El~Filali,
``A novel MRI diagnosis method for brain tumor classification based on deep learning,''
\emph{Healthcare}, vol.~10, no.~3, p.~494, 2022.
doi: 10.3390/healthcare10030494.

\bibitem{shinohara2014}
R.~T.~Shinohara, E.~M.~Sweeney, J.~Goldsmith, N.~Shiee, F.~J.~Mateen, P.~A.~Calabresi,
S.~Jarso, D.~L.~Pham, D.~S.~Reich, and C.~M.~Crainiceanu,
``Statistical normalization techniques for magnetic resonance imaging,''
\emph{NeuroImage: Clinical}, vol.~6, pp.~9--19, 2014.
doi: 10.1016/j.nicl.2014.08.008.


\bibitem{cortes1995support}
C.~Cortes and V.~Vapnik,
``Support-vector networks,''
\emph{Machine Learning}, vol.~20, no.~3, pp.~273--297, 1995.
doi: 10.1007/BF00994018.

\bibitem{pedregosa2011scikit}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
D.~Cournapeau, M.~Brucher, M.~Perrot, and {\'E}.~Duchesnay,
``Scikit-learn: Machine learning in {P}ython,''
\emph{Journal of Machine Learning Research}, vol.~12, pp.~2825--2830, 2011.
[Online]. Available: \url{https://www.jmlr.org/papers/v12/pedregosa11a.html}.



\end{thebibliography}
\vspace{12pt}

\end{document}



