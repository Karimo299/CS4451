\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{placeins} 
\usepackage{url}  
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Brain Tumor Classification from MRI Radiomic and Image Features\\}


\author{
\begin{minipage}{0.24\textwidth}
\centering
\footnotesize
\textbf{Ahmad Alashoury}\\
Department of Computer Science\\
Western University\\
London, Ontario\\
aalashou@uwo.ca
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\footnotesize
\textbf{Christopher Betancur}\\
Department of Computer Science\\
Western University\\
London, Ontario\\
cbetancu@uwo.ca
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\footnotesize
\textbf{Otilia Pasculescu}\\
Department of Computer Science\\
Western University\\
London, Ontario\\
opascule@uwo.ca
\end{minipage}
\hfill
\begin{minipage}{0.24\textwidth}
\centering
\footnotesize
\textbf{Kareem}\\
Department of Computer Science\\
Western University\\
London, Ontario\\
email@uwo.ca
\end{minipage}
}

\maketitle


\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}


\section{Background \& Related Work}

For supervised classification studies for Brain MRI slices, it is common to use publically defined datasets and in this paper we use the Kaggle Brain Tumor dataset. This dataset provides 2D MRI slices with corresponding labels and precomputed first- and second-order radiomic features~\cite{bohajuBrainTumor}.
 

Most of the recent literature on the topic of brain tumour classification from MRI scans utilizes deep convolutional neural networks (CNNs) applied directly to 2D slices.
Abiwinanda et al.\ train a CNN on grayscale brain MRIs that are resized to small
spatial resolutions (e.g., $64\times64$), which shows that even smaller representations of the image, produce good performance~\cite{abiwinanda2019}. Ait~Amou et al.\ propose a deep-learning MRI diagnosis pipeline for brain tumours and explicitly adopt a similar resizing strategy, citing
Abiwinanda et al.\ as motivation~\cite{aitamou2022}. Ait~Amou et al.\ introduces a CNN-based MRI diagnosis method for brain tumor classification where it too uses resized images to feed the CNN, citing Abiwinanda et al.\ as motivation~\cite{aitamou2022}. These papers demonstrate that resized MRI slices can be used to achieve a strong performance while reducing computational cost, but note that they focus only on image-only inputs and do not examine any extra precomputed features based on the MRI scan.

Along with resizing, there are other preprocessing techniques and methods that are used in MRI-based studies. Shinohara et al.\ argue that intensity across many MRI scanners and acquisition protocols may vary in intensity and are never consisentent and due to that normalization is an important step to reduce variation before modeling~\cite{shinohara2014}.
This supports our use of converting the images to grayscale, resizing, and intensity
normalization before feeding the compressed image into the model, alongside standardization of tabular features.

Cortes and Vapnik introduced SVMs as margin-based classifiers that can learn
non-linear decision boundaries via kernel functions such as the radial basis
function (RBF) kernel~\cite{cortes1995support}. 

To apply our models, both SVMs use scikit-learn library which provides standard implementations of classifiers, feature scaling, and cross-validation procedures~\cite{pedregosa2011scikit}.






\section{Methods}

\subsection{Research Objectives}


In this paper we test the following hypothesis:
\begin{quote}
Across the feature subsets we consider, models that combine
image-based MRI features with tabular features achieve higher 
performance in classifying tumour vs non-tumour MRI scans than models
that use a single feature type (tabular-only or image-only), and
for any given feature subset, an RBF-kernel support vector machine (SVM)
performs better the logistic regression model.
\end{quote}

To investigate this hypothesis, we define the following objectives:

\textbf{O1:} Construct seven datasets from the Kaggle Brain Tumor data~\cite{bohajuBrainTumor}
representing different feature subsets: tabular-only (all 13 features, first-order only,
second-order only), image-only features, image + first-order features,
image + second-order features, and image + all tabular features.

\textbf{O2:} Train and tune logistic regression and RBF-kernel SVM classifiers on
each subset using scikit-learn pipelines with standardization, splitting the
data into training and test sets and optimizing hyperparameters via
cross-validation with grid search.

\textbf{O3:} Compare classification performance across feature subsets and models
using held-out test accuracy and macro-averaged F1-score, and analyze which
feature subset and which model (logistic vs SVM) perform best.


\subsection{Research Methodology}

\subsubsection{Dataset and Preprocessing}

We use the Brain Tumor dataset from Kaggle~\cite{bohajuBrainTumor}, which provides
a CSV file with the precomputed radiomic features and corresponding MRI slice
filenames that those features belong to, along with the raw brain MRI images themselves. 

The raw CSV contains $N = 3762$ samples. We check for and drop any rows with
features taking infinite values, but in this dataset no rows are removed. All feature vectors in the dataset have one to one correspondence to a 2D MRI slice along with a binary label \texttt{Class} where label 1 means that the MRI slice contains a tumor and 0 means the MRI slice does not contain a tumor. For each row in the CSV, it includes an \texttt{Image} identifier (to link row of features to image), the class (tumor or no tumor), and 13 tabular features where the features themselves are divided into two types: 
\paragraph{First-order features.}
\begin{itemize}
    \item Mean
    \item Variance
    \item Standard deviation
    \item Skewness
    \item Kurtosis
\end{itemize}

\paragraph{Second-order (texture) features.}
\begin{itemize}
    \item Contrast
    \item Energy
    \item ASM (angular second moment)
    \item Entropy
    \item Homogeneity
    \item Dissimilarity
    \item Correlation
    \item Coarseness
\end{itemize}


So we now construct seven aligned datasets from the same $N$ samples:

\begin{enumerate}
    \item \textbf{D1: First-order only (tabular)} \\
    Utilize the 5 first-order intensity statistics:
    Mean, Variance, Standard Deviation, Skewness, Kurtosis.

    \item \textbf{D2: Second-order only (tabular)} \\
    Utilize the 8 second-order texture features:
    Contrast, Energy, ASM, Entropy, Homogeneity, Dissimilarity, Correlation, Coarseness.

    \item \textbf{D3: All tabular features} \\
    Utilize all 13 tabular features (first-order + second-order).

    \item \textbf{D4: Image only} \\
    Utilize only the grayscale MRI slice, resized to $64\times64$ and normalized.

    \item \textbf{D5: Image + first-order} \\
    Concatenate image pixels with the 5 first-order features.

    \item \textbf{D6: Image + second-order} \\
    Concatenate image pixels with the 8 second-order features.

    \item \textbf{D7: Image + all tabular features} \\
    Concatenate image pixels with all 13 tabular features.
\end{enumerate}

Each MRI slice is loaded in grayscale, resized to a resolution of $64\times64$,
and then normalized to the range $[0, 1]$. By reducing the size of the original MRI slice from $240\times 240$ to $64\times 64$ we reduce the dimensionality (the number of features we pass into the model) while still retaining the overall structural information needed for tumor classification, following prior work that successfully used small resized MRIs for CNN-based brain tumor classification~\cite{abiwinanda2019, aitamou2022}. In MRI analysis, it is common to normalize the intensities so that MRI machine settings have comparable brightness and contrast before we fit any models~\cite{shinohara2014}.

Let $\mathbf{x}_{\text{img}} \in \mathbb{R}^{4096}$ be the flattened representation
(of length $64\times64$) of the compressed and normalized grayscale MRI slice in pixels and $\mathbf{x}_{\text{tab}} \in \mathbb{R}^{13}$ represent the full tabular feature vector for a given MRI slice. 

For each dataset variant defined above, we construct a single feature vector $\mathbf{x}$ from
these components as follows:
\begin{itemize}
    \item tabular-only variants (D1--D3): $\mathbf{x} \subseteq \mathbf{x}_{\text{tab}}$ where $\mathbf{x}$ can contain all tabular features or a subset.
    \item image-only variant (D4): $\mathbf{x} = \mathbf{x}_{\text{img}}$.
    \item Combined variants (D5--D7):
          \[
              \mathbf{x} =
              \begin{bmatrix}
                  \mathbf{x}_{\text{img}} \\
                  \mathbf{x}'
              \end{bmatrix},
          \]
          where $\mathbf{x}'$ as a subvector of
      $\mathbf{x}_{\text{tab}}$ containing either the 5 first-order features,
      the 8 second-order features, or all 13 tabular features.
\end{itemize}

The goal is to learn a function (model) that, given a feature vector $\mathbf{x}$,
outputs a binary label (a standard supervised binary classification task):
\[
  f : \mathcal{X} \to \{0, 1\},
\]
where $\mathcal{X}$ denotes the space of all feature vectors constructed as above.




\subsubsection{Logistic Regression Model}

\subsubsection{Support Vector Machine Model}

SVMs are margin-based classifiers that finds the decision boundary which maximizes the distance between classes ~\cite{cortes1995support}. This model can represent non-linear decision boundaries by implicitly mapping the input features into a higher-dimensional
space through the kernel function. In this paper, we utilize and SVM with a radial basis function (RBF) kernel~\cite{cortes1995support} as our main model,
implemented using the machine learning library scikit-learn~\cite{pedregosa2011scikit}.

For each of the seven datasets D1--D7 we have defined above each with a unique feature configuration, we then train a separate RBF SVM model on the corresponding feature vectors $\mathbf{x}$. To avoid features with large values dominating the kernel, we standardize
all input features to zero mean and unit variance using a \texttt{StandardScaler} inside a scikit-learn \texttt{Pipeline}.

The RBF SVM has two key hyperparameters:
\begin{enumerate}
    \item Regularization parameter $C$ which controls the trade-off between having a wider margin with more training errors versus fitting the training data more strictly with fewer margin violations.
    \item Kernel width parameter $\gamma$ controls the influence of a single training point.
\end{enumerate}



\subsubsection{Training Procedure and Experimental Design}

For each dataset variant D1--D7, we follow the same training and evaluation
protocol for both logistic regression and SVM models. We split each dataset
into 80\% training and 20\% testing using
\texttt{train\_test\_split(..., test\_size=0.2, stratify=y, random\_state=42)}
to preserve the tumour / non-tumour class proportions and ensure
reproducibility.

To stabilise optimisation and make features comparable in scale, we implement
each classifier as a scikit-learn pipeline~\cite{pedregosa2011scikit}. For
logistic regression, the pipeline uses a median imputer, a
\texttt{StandardScaler}, and a \texttt{LogisticRegression} model with
$\ell_1$ or $\ell_2$ regularisation. For the SVM, the pipeline uses a
\texttt{StandardScaler} and an RBF-kernel \texttt{SVC} classifier. In both
models, the scaler operates on the full feature vector $\mathbf{x}$
(tabular features and, when present, flattened image pixels), so that each
feature dimension has approximately zero mean and unit variance.

Hyperparameters are tuned on the training set using stratified
$k$-fold cross-validation with $k=3$, shuffling, and a fixed
\texttt{random\_state}. For logistic regression, the grid search explores
regularisation strengths $C$, penalties ($\ell_1$ vs.\ $\ell_2$), and
compatible solvers (\texttt{liblinear}, \texttt{lbfgs}, \texttt{saga}). For
the SVM, the grid search varies the regularisation parameter $C$ and the RBF
kernel width $\gamma$ over a small logarithmic grid, with the kernel type
fixed to RBF.

In both models, we select the hyperparameter combination that maximises mean
cross-validation accuracy on the training data, retrain the best model on the
full training set, and evaluate it once on the held-out test set. For every
model (logistic vs.\ SVM) and dataset D1--D7, we report test accuracy and
macro-averaged F1-score to compare performance across feature subsets and
classifiers.


\subsubsection{Evaluation Metrics}
For a given model (logistic regression or RBF SVM), we train and tune
hyperparameters and then evaluate performance using three quantities:

\begin{enumerate}
    \item Mean cross-validated accuracy on the training split
          (used only for hyperparameter selection),
    \item Overall accuracy on the held-out test set,
    \item Macro-averaged F1-score on the held-out test set.
\end{enumerate}

To select hyperparameters, we use mean cross-validated accuracy: for each
hyperparameter configuration we run $k$-fold cross-validation on the training
data and compute the average accuracy over the $k$ train/validation splits.
This provides a more stable estimate of generalisation than a single
train/validation split and helps reduce overfitting of hyperparameters.

After choosing the best hyperparameters, we retrain the model on the full
training split and evaluate it once on the held-out test set. On this test
set we report overall accuracy and macro-averaged F1-score to compare how
well each model performs across the different datasets.

We evaluate test performance by comparing the predicted label $\hat{y}$ with
the true label $y$ for every MRI slice. We treat the tumour class as positive
($y=1$) and the no-tumour class as negative ($y=0$), and define:

\begin{itemize}
    \item $\mathrm{TP}$ (true positives): slices where $y=1$ and $\hat{y}=1$
          (tumour correctly predicted),
    \item $\mathrm{TN}$ (true negatives): slices where $y=0$ and $\hat{y}=0$
          (no tumour correctly predicted),
    \item $\mathrm{FP}$ (false positives): slices where $y=0$ but $\hat{y}=1$
          (model predicts a tumour where there is none),
    \item $\mathrm{FN}$ (false negatives): slices where $y=1$ but $\hat{y}=0$
          (model misses a true tumour).
\end{itemize}

The overall \emph{accuracy} is the fraction of correct predictions:
\[
\text{Accuracy}
= \frac{\mathrm{TP} + \mathrm{TN}}
       {\mathrm{TP} + \mathrm{TN} + \mathrm{FP} + \mathrm{FN}}.
\]

For each class $c \in \{\text{tumour}, \text{no-tumour}\}$ we compute
precision, recall, and F1-score. For the positive (tumour) class these are
\[
\text{Precision}_+ =
\frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}},
\qquad
\text{Recall}_+ =
\frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}},
\]
\[
\text{F1}_+ = 2 \cdot
\frac{\text{Precision}_+ \cdot \text{Recall}_+}
     {\text{Precision}_+ + \text{Recall}_+}.
\]
Analogous quantities $\text{Precision}_-$, $\text{Recall}_-$, and
$\text{F1}_-$ are defined for the no-tumour class. The macro-averaged
F1-score reported in our experiments corresponds to scikit-learn’s
\texttt{f1\_score(y\_test, y\_pred, average="macro")}, which simply computes
\[
\text{F1}_{\text{macro}} = \tfrac{1}{2} \big( \text{F1}_+ + \text{F1}_- \big),
\]
the unweighted mean of the two class-wise F1-scores.


\subsubsection{Threats to Validity and Rationale}

Our study has several limitations that affect how broadly the results can be interpreted.  
First, all experiments use only a single Kaggle dataset~\cite{bohajuBrainTumor} with precomputed radiomic features and 2D slices. How that data was computed and what was used are not fully documented, so performance estimations in this paper may not be able generalize to other scanners, institutions, or clinical populations.

Second, we work at the slice level and do not have patient identifiers. If multiple
MRI slices come from the same patient appear in both the training and test splits, this may
inflate reported accuracy and F1-scores because the model effectively sees very
similar images during training and testing. Since, we there is no provided documentation to verify whether a given MRI slice has a tumor or not, we simply trust that the provided labels are true.

Third, our models are restricted to logistic regression and RBF-kernel SVMs with
relatively small hyperparameter grids. More complex architectures (e.g., CNNs) or
richer hyperparameter searches could change the absolute performance and possibly the
relative ranking of feature configurations. In addition, resizing MRIs to $64\times64$
and flattening them into vectors may discard fine-grained spatial information.

Despite these limitations, our design is appropriate for O1–O3 because it lets us compare
the various feature subsets we have defined above and compare logistic regression versus SVM models. We are able to apply an 80/20 stratified split with cross-validated model selection, and two classifiers provide a clear test of our hypothesis, although the resulting metrics should not be viewed as definitive clinical performance.

\section{Results}

\subsection{System Requirements and Architecture Outcomes}

The experimental pipeline was designed with the following requirements:



\textbf{Consistency across datasets:} All models must use the same aligned
    sample ordering to allow direct dataset-to-dataset comparisons. The implementation
    meets this requirement by constructing all feature matrices (D1–D7) from a shared
    cleaned DataFrame and applying a single stratified train/test split.
    
\textbf{Scalability:} Image preprocessing is performed once and cached.
    Subsequent runs reuse the stored feature matrix, ensuring reproducible and efficient
    execution across experiments.
    
\textbf{Model robustness:} Logistic regression must remain numerically stable
    even in high-dimensional settings (e.g., D7 with 4109 features). Our implementation
    using the \texttt{lbfgs} and \texttt{liblinear} solvers satisfies this requirement,
    with all models converging without numerical instability.
    

\subsection{Implementation, Testing, and Verification}

During implementation, several aspects of the system were verified:

\textbf{Image–tabular alignment:} Each image feature vector was correctly 
    matched with its corresponding radiomic features. Consistency checks using sampled
    rows confirmed correct alignment across all 3762 samples.
    
\textbf{Feature normalization:} Standardization was applied jointly over
    image and tabular dimensions. Logistic regression performed substantially worse
    without scaling, confirming the necessity of normalization for stable optimization.
    
\textbf{Cross-validation behavior:} Grid search using stratified 3-fold CV
    produced consistent scores across folds, indicating reliable model behavior and
    low sensitivity to sampling variation.



\subsection{Logistic Regression Results Analysis}

Logistic regression was trained and tuned independently on each dataset variant
(D1–D7), representing different combinations of radiomic features and image-based
features. Across all experiments, D7 (Image + All Tabular Features) achieved the best
performance, with a test accuracy of 0.9934 and a macro F1-score of 0.9933.
Conversely, the weakest performance was observed in D1 (First-order only) and D4
(Image-only), both of which lack higher-level texture information that appears essential
for robust tumor classification.

Fig.~\ref{fig:lr_bar_graph} summarizes the performance across D1–D7.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{lr_bar_graph.png}
    \caption{Logistic regression test accuracy and macro F1-score across feature subsets D1–D7.
    Performance improves as additional radiomic or image features are incorporated,
    with the combined image + tabular feature set (D7) performing best.}
    \label{fig:lr_bar_graph}
\end{figure}

\subsection{SVM Results Analysis}
SVM was trained and tested on each dataset variant (D1–D7), representing different combinations of radiomic features and image-based features. The best test accuracy result comes from the D3 data, with a score of 0.980, and also a very high CV score as well (0.986). The best F1 score is also from the D3 data, with a score of 0.991. Across all experiments, D3 achieved the best overall performance. The weakest performance was observed in D4 (image only) with a test accuracy of 0.896, along with a low CV score and a low F1 score. However, the lowest F1 score was reported in D1 with a score of 0.885.

Fig.~\ref{fig:svm_bar_plot.png} summarizes the performance across D1–D7.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.48\textwidth]{svm_bar_graph.png}
    \caption{SVM test accuracy and macro F1-score across feature subsets D1–D7.
    Performance improves as additional radiomic or image features are incorporated,
    with the combined image + tabular feature set (D7) performing best.}
    \label{fig:svm_bar_plot}
\end{figure}

\subsection{Tabulated Results}

Tables~\ref{tab:logreg_results} and~\ref{tab:svm_results} list the cross-validated accuracy, test accuracy, and macro F1-score obtained for each dataset using logistic regression and SVM, respectively.

\begin{table}[H]
\centering
\caption{Logistic Regression Performance Across D1--D7}
\label{tab:logreg_results}
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{@{}lccc@{}}
\hline
\textbf{Dataset} & \textbf{CV Accuracy} & \textbf{Test Accuracy} & \textbf{Macro F1} \\
\hline
D1 -- First-order only        & 0.885 & 0.885 & 0.883 \\
D2 -- Second-order only       & 0.977 & 0.984 & 0.984 \\
D3 -- All tabular features    & 0.981 & 0.991 & 0.991 \\
D4 -- Image only (64$\times$64) & 0.883 & 0.888 & 0.887 \\
D5 -- Image + first-order     & 0.955 & 0.975 & 0.975 \\
D6 -- Image + second-order    & 0.976 & 0.985 & 0.985 \\
D7 -- Image + all tabular     & 0.979 & 0.993 & 0.993 \\
\hline
\end{tabular}
\end{table}



\begin{table}[H]
\centering
\caption{SVM Performance Across D1--D7}
\label{tab:svm_results}
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{@{}lccc@{}}
\hline
\textbf{Dataset} & \textbf{CV Accuracy} & \textbf{Test Accuracy} & \textbf{Macro F1} \\
\hline
D1 -- First-order only        & 0.946 & 0.944 & 0.885 \\
D2 -- Second-order only       & 0.982 & 0.971 & 0.987 \\
D3 -- All tabular features    & 0.986 & 0.980 & 0.991 \\
D4 -- Image only (64$\times$64) & 0.839 & 0.896 & 0.911 \\
D5 -- Image + first-order     & 0.845 & 0.912 & 0.925 \\
D6 -- Image + second-order    & 0.847 & 0.906 & 0.925 \\
D7 -- Image + all tabular     & 0.850 & 0.919 & 0.965 \\
\hline
\end{tabular}
\end{table}



\subsection{Result Interpretations - logistic regression}

\textbf{First-order features alone are limited (D1).} These features encode only global intensity statistics and therefore fail to capture local tumor structure.

\textbf{Second-order texture features substantially improve performance (D2).}
Texture descriptors such as contrast, entropy, and homogeneity encode spatial
irregularities that are highly characteristic of tumor presence.

\textbf{Image-only inputs perform poorly with linear models (D4).}
Because logistic regression is a linear classifier, it cannot efficiently model
complex spatial patterns present in raw pixel data when the image is flattened.

\textbf{Combining image and tabular features yields the strongest results (D5–D7).}
The combination provides both high-level radiomic summaries and fine-grained pixel
cues, leading to the highest-performing model (D7).

\subsection{Result Interpretations - SVM}

\textbf{Tabular features are strong:} All tabular datasets outperform image datasets in test accuracy, which could indicate that SVM performs better with tabular data than it does with image data with this specific model and dataset. D3 (all tabular features) is also the best performer, which strengthens the indication that tabular features outperform non-tabular features in this evaluation.

\textbf{Image data performs poorly: }D4 (image only) is observed as the weakest performer in test accuracy, which indicates that images alone are not strong. When tabular data is added, performance rises. This shows us that tabular data strengthens the model.

\textbf{First-order features are not enough: }D1 is also observed as a weak performer and it only takes into account first-order features. This is an indication that first-order features are not enough to detect brain tumors and the data itself may not descriptive enough for the model to be accurate in this assessment.

\subsection{Comparison of Logistic Regression and SVM}

Table~\ref{tab:lr_svm_comparison} directly compares logistic regression and
RBF SVM on each dataset D1--D7 using test accuracy and macro F1-score.

Logistic regression generally achieves higher test accuracy on the
tabular-only and fused image+tabular datasets (D2, D3, D5--D7), while SVM
is competitive or slightly better on some macro F1-scores (e.g., D3 and D7)
and on the image-only configuration (D4). In particular, the best overall
test accuracy is obtained by logistic regression on D7 (image + all tabular
features), whereas the best SVM configuration is D3 (all tabular features).
This means that our original hypothesis that an RBF SVM would consistently
outperform logistic regression on every feature subset is \emph{not}
supported by the empirical results: for many radiomic-rich settings, a
well-regularized linear model is at least as strong as the non-linear SVM.

\begin{table}[H]
\centering
\caption{Comparison of Logistic Regression and SVM Across D1--D7}
\label{tab:lr_svm_comparison}
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{@{}lcccc@{}}
\hline
\textbf{Dataset} & \textbf{LR Acc} & \textbf{LR Macro F1} &
\textbf{SVM Acc} & \textbf{SVM Macro F1} \\
\hline
D1 -- First-order only        & 0.885 & 0.883 & 0.944 & 0.885 \\
D2 -- Second-order only       & 0.984 & 0.984 & 0.971 & 0.987 \\
D3 -- All tabular features    & 0.991 & 0.991 & 0.980 & 0.991 \\
D4 -- Image only (64$\times$64) & 0.888 & 0.887 & 0.896 & 0.911 \\
D5 -- Image + first-order     & 0.975 & 0.975 & 0.912 & 0.925 \\
D6 -- Image + second-order    & 0.985 & 0.985 & 0.906 & 0.925 \\
D7 -- Image + all tabular     & 0.993 & 0.993 & 0.919 & 0.965 \\
\hline
\end{tabular}
\end{table}

\FloatBarrier   % keeps TABLE III before the Novelty subsection


\subsection{Novelty of Our Approach}

Two aspects of novelty characterize our methodology:

\begin{enumerate}
    \item \textbf{A unified evaluation of seven radiomic–image feature subsets.}
    While prior work often focuses solely on image-based deep learning models, our 
    experiments demonstrate that radiomic features alone can achieve near-perfect 
    performance with both linear regression and SVM implementation.

    \item \textbf{Fusion of high-dimensional pixel data with engineered features.}
    Few studies evaluate logistic regression on a combined 4096-pixel + 13-feature
    input space. Our results show that with proper standardization and regularization,
    logistic regression can remain stable and highly accurate even in this setting.
\end{enumerate}

Overall, the experiments confirm that radiomic features—
particularly texture descriptors—carry strong predictive signals for tumor
classification.


\section{Conclusions \& Future Work}

This project set out to investigate three objectives (O1--O3): (1) construct
seven aligned datasets combining radiomic and image features (D1--D7),
(2) train and tune logistic regression and RBF SVM models on each subset
using a common pipeline, and (3) compare performance across feature
configurations and models using held-out test accuracy and macro F1-score.

With respect to O1, we successfully built seven consistent feature
configurations from the Kaggle Brain Tumor dataset, ranging from simple
first-order radiomic features (D1) to a fused representation of
64$\times$64 image pixels and all 13 tabular features (D7). For O2, both
logistic regression and SVM were implemented in standardized
scikit-learn pipelines with stratified train/test splits and 3-fold
cross-validated grid search. The resulting models converged reliably on all
datasets, including the high-dimensional fused representations.

Regarding O3 and our hypothesis, the results show that combining image and
tabular features does indeed produce the strongest models overall: for both
classifiers, the best-performing configurations involve either all tabular
features (D3) or image+tabular fusion (D5--D7). However, the second part of
our hypothesis, that an RBF SVM would uniformly outperform logistic
regression on each feature subset, is not supported. Logistic regression
matches or exceeds SVM in test accuracy on most tabular-rich datasets,
including the best overall configuration D7, while SVM is only clearly
superior for the image-only case and some macro F1-scores. A key conclusion
is that carefully regularized linear models remain highly competitive for
radiomic-based brain tumor classification, especially when informative
texture features are available.

Several avenues for future work arise from this study. First, we worked at
the slice level without patient identifiers; a natural extension is to
reconstruct patient-level splits to avoid potential information leakage and
to evaluate per-patient rather than per-slice performance. Second, our
models are limited to logistic regression and RBF SVMs; exploring modern
deep learning architectures (e.g., CNNs for the images plus MLP heads for
tabular features) could reveal whether more complex models still gain
significant accuracy beyond the radiomic baselines. Third, our
hyperparameter grids were deliberately small; more extensive searches,
calibration analysis, and class-imbalance handling (e.g., focal losses or
cost-sensitive learning) could further refine model performance. Finally,
explainability methods such as feature importance analysis or SHAP values
could be applied to better understand which radiomic descriptors drive
classification decisions.

From an implementation perspective, this project also produced several
practical lessons. We found that strict alignment between image files and
CSV rows is critical; even small indexing mistakes can silently corrupt
supervised labels. Normalization and standardization are indispensable for
both logistic regression and SVM, particularly when fusing pixel values and
radiomic features in a single high-dimensional vector. Caching preprocessed
image features made repeated experimentation feasible and highlights the
importance of engineering the pipeline, not just tuning the model. More
broadly, the work reinforced that simple, well-validated baselines are
essential: they provide strong performance, expose bugs early, and offer a
clear reference point for any future, more complex models to beat.


\begin{thebibliography}{00}

\bibitem{bohajuBrainTumor}
J.~Bohaju, ``Brain Tumor,'' Kaggle, Jul. 2020, ver.~3. [Online]. Available:
\url{https://www.kaggle.com/datasets/jakeshbohaju/brain-tumor}.
doi: 10.34740/KAGGLE/DSV/955413. [Accessed: Dec. 6, 2025].

\bibitem{abiwinanda2019}
N.~Abiwinanda, M.~Hanif, S.~T.~Hesaputra, A.~Handayani, and T.~R.~Mengko,
``Brain tumor classification using convolutional neural network,''
in \emph{World Congress on Medical Physics and Biomedical Engineering 2018}.
Singapore: Springer, 2019, pp.~183--189.
doi: 10.1007/978-981-10-9035-6\_33.

\bibitem{aitamou2022}
M.~Ait~Amou, H.~Xia, A.~Larhmam, and M.~Alami~El~Filali,
``A novel MRI diagnosis method for brain tumor classification based on deep learning,''
\emph{Healthcare}, vol.~10, no.~3, p.~494, 2022.
doi: 10.3390/healthcare10030494.

\bibitem{shinohara2014}
R.~T.~Shinohara, E.~M.~Sweeney, J.~Goldsmith, N.~Shiee, F.~J.~Mateen, P.~A.~Calabresi,
S.~Jarso, D.~L.~Pham, D.~S.~Reich, and C.~M.~Crainiceanu,
``Statistical normalization techniques for magnetic resonance imaging,''
\emph{NeuroImage: Clinical}, vol.~6, pp.~9--19, 2014.
doi: 10.1016/j.nicl.2014.08.008.


\bibitem{cortes1995support}
C.~Cortes and V.~Vapnik,
``Support-vector networks,''
\emph{Machine Learning}, vol.~20, no.~3, pp.~273--297, 1995.
doi: 10.1007/BF00994018.

\bibitem{pedregosa2011scikit}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
D.~Cournapeau, M.~Brucher, M.~Perrot, and {\'E}.~Duchesnay,
``Scikit-learn: Machine learning in {P}ython,''
\emph{Journal of Machine Learning Research}, vol.~12, pp.~2825--2830, 2011.
[Online]. Available: \url{https://www.jmlr.org/papers/v12/pedregosa11a.html}.



\end{thebibliography}
\vspace{12pt}

\end{document}
