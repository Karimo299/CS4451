{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV,\n",
        "    cross_val_score,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw dataset shape: (1644, 19)\n",
            "    Image       Mean     Variance  Standard Deviation   Entropy  Skewness  \\\n",
            "0  Image1  23.448517  2538.985627           50.388348  0.651174  1.984202   \n",
            "1  Image2   4.398331   834.853030           28.893823  0.953532  6.495203   \n",
            "\n",
            "    Kurtosis    Contrast    Energy       ASM  Homogeneity  Dissimilarity  \\\n",
            "0   5.421042  181.467713  0.781557  0.610831     0.847033       2.765411   \n",
            "1  43.349355   76.745886  0.972770  0.946281     0.980762       0.548605   \n",
            "\n",
            "   Correlation     Coarseness        PSNR      SSIM       MSE        DC  \\\n",
            "0     0.968576  7.458341e-155   97.974630  0.777011  0.171163  0.303989   \n",
            "1     0.959751  7.458341e-155  110.346597  0.977953  0.009913  0.839019   \n",
            "\n",
            "   Target  \n",
            "0       1  \n",
            "1       1  \n",
            "\n",
            "First order features: ['Mean', 'Variance', 'Standard Deviation', 'Skewness', 'Kurtosis']\n",
            "Second order features: ['Contrast', 'Energy', 'ASM', 'Entropy', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']\n",
            "\n",
            "Available features in dataset: ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness', 'PSNR', 'SSIM', 'MSE', 'DC']\n"
          ]
        }
      ],
      "source": [
        "RAW_DATA_PATH = \"data/bt_dataset_t3.csv\"\n",
        "IMAGES_DIR = \"data/images\"\n",
        "IMAGE_SIZE = (64, 64)\n",
        "\n",
        "# Load raw dataset\n",
        "df_raw = pd.read_csv(RAW_DATA_PATH)\n",
        "print(f\"Raw dataset shape: {df_raw.shape}\")\n",
        "print(df_raw.head(2))\n",
        "\n",
        "# Define feature categories\n",
        "FIRST_ORDER_FEATURES = [\"Mean\", \"Variance\", \"Standard Deviation\", \"Skewness\", \"Kurtosis\"]\n",
        "SECOND_ORDER_FEATURES = [\"Contrast\", \"Energy\", \"ASM\", \"Entropy\", \"Homogeneity\", \"Dissimilarity\", \"Correlation\", \"Coarseness\"]\n",
        "\n",
        "# Verify all features exist in the dataset\n",
        "all_features = FIRST_ORDER_FEATURES + SECOND_ORDER_FEATURES\n",
        "available_features = [col for col in df_raw.columns if col not in [\"Image\", \"Target\"]]\n",
        "print(f\"\\nFirst order features: {FIRST_ORDER_FEATURES}\")\n",
        "print(f\"Second order features: {SECOND_ORDER_FEATURES}\")\n",
        "print(f\"\\nAvailable features in dataset: {available_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for data preparation and model training\n",
        "\n",
        "def prepare_tabular_data(df, feature_list):\n",
        "    \"\"\"Prepare tabular data with specified features.\"\"\"\n",
        "    X = df[feature_list].copy()\n",
        "    y = df[\"Target\"].copy()\n",
        "    \n",
        "    # Remove rows with infinite values\n",
        "    inf_mask = np.isinf(X).any(axis=1)\n",
        "    if inf_mask.any():\n",
        "        print(f\"Removing {inf_mask.sum()} rows containing infinite values.\")\n",
        "        X = X.loc[~inf_mask].reset_index(drop=True)\n",
        "        y = y.loc[~inf_mask].reset_index(drop=True)\n",
        "    \n",
        "    # Fill missing values\n",
        "    X = X.fillna(X.median(numeric_only=True))\n",
        "    \n",
        "    print(f\"Tabular data shape: {X.shape}\")\n",
        "    print(f\"Features used: {feature_list}\")\n",
        "    print(f\"Class counts:\\n{y.value_counts()}\\n\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=IMAGE_SIZE):\n",
        "    \"\"\"Load and preprocess an image.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        if img.mode != \"L\":\n",
        "            img = img.convert(\"L\")\n",
        "        img = img.resize(target_size)\n",
        "        return (np.array(img, dtype=np.float32) / 255.0).flatten()\n",
        "    except Exception as exc:\n",
        "        print(f\"Failed to load {image_path}: {exc}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_combined_dataset(df, feature_list, images_dir=IMAGES_DIR, target_size=IMAGE_SIZE):\n",
        "    \"\"\"Build dataset combining tabular features with image features.\"\"\"\n",
        "    X_tab = df[feature_list].copy()\n",
        "    y = df[\"Target\"].copy()\n",
        "    \n",
        "    # Remove rows with infinite values\n",
        "    inf_mask = np.isinf(X_tab).any(axis=1)\n",
        "    if inf_mask.any():\n",
        "        print(f\"Removing {inf_mask.sum()} rows with inf values.\")\n",
        "        df = df.loc[~inf_mask].reset_index(drop=True)\n",
        "        X_tab = X_tab.loc[~inf_mask].reset_index(drop=True)\n",
        "        y = y.loc[~inf_mask].reset_index(drop=True)\n",
        "    \n",
        "    X_tab = X_tab.fillna(X_tab.median(numeric_only=True))\n",
        "    \n",
        "    # Load image features\n",
        "    image_features = []\n",
        "    valid_indices = []\n",
        "    for idx, image_name in enumerate(df[\"Image\"]):\n",
        "        if not isinstance(image_name, str):\n",
        "            continue\n",
        "        filename = image_name if image_name.endswith(\".jpg\") else f\"{image_name}.jpg\"\n",
        "        path = os.path.join(images_dir, filename)\n",
        "        if not os.path.exists(path):\n",
        "            continue\n",
        "        features = load_and_preprocess_image(path, target_size)\n",
        "        if features is not None:\n",
        "            image_features.append(features)\n",
        "            valid_indices.append(idx)\n",
        "    \n",
        "    if not image_features:\n",
        "        raise ValueError(\"No images could be processed.\")\n",
        "    \n",
        "    X_tab_valid = X_tab.iloc[valid_indices].reset_index(drop=True)\n",
        "    y_valid = y.iloc[valid_indices].reset_index(drop=True)\n",
        "    X_image = np.vstack(image_features)\n",
        "    X_combined = np.hstack([X_tab_valid.values, X_image])\n",
        "    \n",
        "    print(f\"Combined dataset: {X_combined.shape[0]} samples, {X_tab_valid.shape[1]} tabular + {X_image.shape[1]} image features\")\n",
        "    print(f\"Features used: {feature_list}\")\n",
        "    print(f\"Class counts:\\n{y_valid.value_counts()}\\n\")\n",
        "    \n",
        "    return X_combined, y_valid\n",
        "\n",
        "\n",
        "def build_image_only_dataset(df, images_dir=IMAGES_DIR, target_size=IMAGE_SIZE):\n",
        "    \"\"\"Build dataset with only image features (no tabular features).\"\"\"\n",
        "    y = df[\"Target\"].copy()\n",
        "    \n",
        "    # Load image features\n",
        "    image_features = []\n",
        "    valid_indices = []\n",
        "    for idx, image_name in enumerate(df[\"Image\"]):\n",
        "        if not isinstance(image_name, str):\n",
        "            continue\n",
        "        filename = image_name if image_name.endswith(\".jpg\") else f\"{image_name}.jpg\"\n",
        "        path = os.path.join(images_dir, filename)\n",
        "        if not os.path.exists(path):\n",
        "            continue\n",
        "        features = load_and_preprocess_image(path, target_size)\n",
        "        if features is not None:\n",
        "            image_features.append(features)\n",
        "            valid_indices.append(idx)\n",
        "    \n",
        "    if not image_features:\n",
        "        raise ValueError(\"No images could be processed.\")\n",
        "    \n",
        "    y_valid = y.iloc[valid_indices].reset_index(drop=True)\n",
        "    X_image = np.vstack(image_features)\n",
        "    \n",
        "    print(f\"Image-only dataset: {X_image.shape[0]} samples, {X_image.shape[1]} image features\")\n",
        "    print(f\"Class counts:\\n{y_valid.value_counts()}\\n\")\n",
        "    \n",
        "    return X_image, y_valid\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, run_name):\n",
        "    \"\"\"Train and evaluate a logistic regression model.\"\"\"\n",
        "    param_grid = [\n",
        "        {\n",
        "            \"classifier__C\": [0.01, 0.1, 1, 10],\n",
        "            \"classifier__penalty\": [\"l1\"],\n",
        "            \"classifier__solver\": [\"liblinear\", \"saga\"],\n",
        "        },\n",
        "        {\n",
        "            \"classifier__C\": [0.01, 0.1, 1, 10],\n",
        "            \"classifier__penalty\": [\"l2\"],\n",
        "            \"classifier__solver\": [\"lbfgs\", \"saga\"],\n",
        "        },\n",
        "    ]\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"classifier\", LogisticRegression(random_state=42, max_iter=1000)),\n",
        "    ])\n",
        "    \n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    print(f\"Train/Test split sizes: {len(X_train)} / {len(X_test)}\")\n",
        "    \n",
        "    search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring=\"accuracy\",\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "    )\n",
        "    \n",
        "    search.fit(X_train, y_train)\n",
        "    print(f\"Best params: {search.best_params_}\")\n",
        "    print(f\"Mean CV accuracy: {search.best_score_:.4f}\")\n",
        "    \n",
        "    best_model = search.best_estimator_\n",
        "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
        "    print(f\"CV mean ± 2*std: {cv_scores.mean():.4f} ± {2 * cv_scores.std():.4f}\")\n",
        "    \n",
        "    y_pred = best_model.predict(X_test)\n",
        "    print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(\"Confusion matrix (test):\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification report (test):\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "    \n",
        "    return best_model, search.best_params_, search.best_score_, accuracy_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 1: Only First Order Features (Tabular Only)\n",
            "======================================================================\n",
            "Tabular data shape: (1644, 5)\n",
            "Features used: ['Mean', 'Variance', 'Standard Deviation', 'Skewness', 'Kurtosis']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 10, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
            "Mean CV accuracy: 0.8829\n",
            "CV mean ± 2*std: 0.8829 ± 0.0075\n",
            "Test accuracy: 0.8754\n",
            "Confusion matrix (test):\n",
            "[[  2  37]\n",
            " [  4 286]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0513    0.0889        39\n",
            "           1     0.8854    0.9862    0.9331       290\n",
            "\n",
            "    accuracy                         0.8754       329\n",
            "   macro avg     0.6094    0.5187    0.5110       329\n",
            "weighted avg     0.8200    0.8754    0.8330       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 1: Only First Order Features (Tabular Only)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 1: Only First Order Features (Tabular Only)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_first, y_first = prepare_tabular_data(df_raw, FIRST_ORDER_FEATURES)\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
        "    X_first, y_first, test_size=0.2, random_state=42, stratify=y_first\n",
        ")\n",
        "\n",
        "model_1, params_1, cv_acc_1, test_acc_1 = train_and_evaluate_model(\n",
        "    X_train_1, X_test_1, y_train_1, y_test_1, \"Run 1: First Order Only\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 2: Only Second Order Features (Tabular Only)\n",
            "======================================================================\n",
            "Tabular data shape: (1644, 8)\n",
            "Features used: ['Contrast', 'Energy', 'ASM', 'Entropy', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 1, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8882\n",
            "CV mean ± 2*std: 0.8882 ± 0.0091\n",
            "Test accuracy: 0.8784\n",
            "Confusion matrix (test):\n",
            "[[  1  38]\n",
            " [  2 288]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0256    0.0476        39\n",
            "           1     0.8834    0.9931    0.9351       290\n",
            "\n",
            "    accuracy                         0.8784       329\n",
            "   macro avg     0.6084    0.5094    0.4913       329\n",
            "weighted avg     0.8182    0.8784    0.8299       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 2: Only Second Order Features (Tabular Only)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 2: Only Second Order Features (Tabular Only)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_second, y_second = prepare_tabular_data(df_raw, SECOND_ORDER_FEATURES)\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
        "    X_second, y_second, test_size=0.2, random_state=42, stratify=y_second\n",
        ")\n",
        "\n",
        "model_2, params_2, cv_acc_2, test_acc_2 = train_and_evaluate_model(\n",
        "    X_train_2, X_test_2, y_train_2, y_test_2, \"Run 2: Second Order Only\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 3: Both First and Second Order Features (Tabular Only)\n",
            "======================================================================\n",
            "Tabular data shape: (1644, 13)\n",
            "Features used: ['Mean', 'Variance', 'Standard Deviation', 'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM', 'Entropy', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 10, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8867\n",
            "CV mean ± 2*std: 0.8867 ± 0.0057\n",
            "Test accuracy: 0.8754\n",
            "Confusion matrix (test):\n",
            "[[  2  37]\n",
            " [  4 286]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0513    0.0889        39\n",
            "           1     0.8854    0.9862    0.9331       290\n",
            "\n",
            "    accuracy                         0.8754       329\n",
            "   macro avg     0.6094    0.5187    0.5110       329\n",
            "weighted avg     0.8200    0.8754    0.8330       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 3: Both First and Second Order Features (Tabular Only)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 3: Both First and Second Order Features (Tabular Only)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_both, y_both = prepare_tabular_data(df_raw, FIRST_ORDER_FEATURES + SECOND_ORDER_FEATURES)\n",
        "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(\n",
        "    X_both, y_both, test_size=0.2, random_state=42, stratify=y_both\n",
        ")\n",
        "\n",
        "model_3, params_3, cv_acc_3, test_acc_3 = train_and_evaluate_model(\n",
        "    X_train_3, X_test_3, y_train_3, y_test_3, \"Run 3: Both Orders\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 4: First Order Features + Images\n",
            "======================================================================\n",
            "Combined dataset: 1644 samples, 5 tabular + 4096 image features\n",
            "Features used: ['Mean', 'Variance', 'Standard Deviation', 'Skewness', 'Kurtosis']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 0.01, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8814\n",
            "CV mean ± 2*std: 0.8814 ± 0.0030\n",
            "Test accuracy: 0.8815\n",
            "Confusion matrix (test):\n",
            "[[  0  39]\n",
            " [  0 290]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        39\n",
            "           1     0.8815    1.0000    0.9370       290\n",
            "\n",
            "    accuracy                         0.8815       329\n",
            "   macro avg     0.4407    0.5000    0.4685       329\n",
            "weighted avg     0.7770    0.8815    0.8259       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 4: First Order Features + Images\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 4: First Order Features + Images\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_combined_4, y_combined_4 = build_combined_dataset(df_raw, FIRST_ORDER_FEATURES)\n",
        "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(\n",
        "    X_combined_4, y_combined_4, test_size=0.2, random_state=42, stratify=y_combined_4\n",
        ")\n",
        "\n",
        "model_4, params_4, cv_acc_4, test_acc_4 = train_and_evaluate_model(\n",
        "    X_train_4, X_test_4, y_train_4, y_test_4, \"Run 4: First Order + Images\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 5: Second Order Features + Images\n",
            "======================================================================\n",
            "Combined dataset: 1644 samples, 8 tabular + 4096 image features\n",
            "Features used: ['Contrast', 'Energy', 'ASM', 'Entropy', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 0.01, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8814\n",
            "CV mean ± 2*std: 0.8814 ± 0.0030\n",
            "Test accuracy: 0.8815\n",
            "Confusion matrix (test):\n",
            "[[  0  39]\n",
            " [  0 290]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        39\n",
            "           1     0.8815    1.0000    0.9370       290\n",
            "\n",
            "    accuracy                         0.8815       329\n",
            "   macro avg     0.4407    0.5000    0.4685       329\n",
            "weighted avg     0.7770    0.8815    0.8259       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 5: Second Order Features + Images\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 5: Second Order Features + Images\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_combined_5, y_combined_5 = build_combined_dataset(df_raw, SECOND_ORDER_FEATURES)\n",
        "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(\n",
        "    X_combined_5, y_combined_5, test_size=0.2, random_state=42, stratify=y_combined_5\n",
        ")\n",
        "\n",
        "model_5, params_5, cv_acc_5, test_acc_5 = train_and_evaluate_model(\n",
        "    X_train_5, X_test_5, y_train_5, y_test_5, \"Run 5: Second Order + Images\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 6: Both First and Second Order Features + Images\n",
            "======================================================================\n",
            "Combined dataset: 1644 samples, 13 tabular + 4096 image features\n",
            "Features used: ['Mean', 'Variance', 'Standard Deviation', 'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM', 'Entropy', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 0.01, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8814\n",
            "CV mean ± 2*std: 0.8814 ± 0.0030\n",
            "Test accuracy: 0.8815\n",
            "Confusion matrix (test):\n",
            "[[  0  39]\n",
            " [  0 290]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        39\n",
            "           1     0.8815    1.0000    0.9370       290\n",
            "\n",
            "    accuracy                         0.8815       329\n",
            "   macro avg     0.4407    0.5000    0.4685       329\n",
            "weighted avg     0.7770    0.8815    0.8259       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 6: Both First and Second Order Features + Images\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 6: Both First and Second Order Features + Images\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_combined_6, y_combined_6 = build_combined_dataset(df_raw, FIRST_ORDER_FEATURES + SECOND_ORDER_FEATURES)\n",
        "X_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(\n",
        "    X_combined_6, y_combined_6, test_size=0.2, random_state=42, stratify=y_combined_6\n",
        ")\n",
        "\n",
        "model_6, params_6, cv_acc_6, test_acc_6 = train_and_evaluate_model(\n",
        "    X_train_6, X_test_6, y_train_6, y_test_6, \"Run 6: Both Orders + Images\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RUN 7: Images Only (No Tabular Features)\n",
            "======================================================================\n",
            "Image-only dataset: 1644 samples, 4096 image features\n",
            "Class counts:\n",
            "Target\n",
            "1    1449\n",
            "0     195\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test split sizes: 1315 / 329\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best params: {'classifier__C': 0.01, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
            "Mean CV accuracy: 0.8814\n",
            "CV mean ± 2*std: 0.8814 ± 0.0030\n",
            "Test accuracy: 0.8815\n",
            "Confusion matrix (test):\n",
            "[[  0  39]\n",
            " [  0 290]]\n",
            "\n",
            "Classification report (test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        39\n",
            "           1     0.8815    1.0000    0.9370       290\n",
            "\n",
            "    accuracy                         0.8815       329\n",
            "   macro avg     0.4407    0.5000    0.4685       329\n",
            "weighted avg     0.7770    0.8815    0.8259       329\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN 7: Images Only (No Tabular Features)\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"RUN 7: Images Only (No Tabular Features)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_image_only, y_image_only = build_image_only_dataset(df_raw)\n",
        "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(\n",
        "    X_image_only, y_image_only, test_size=0.2, random_state=42, stratify=y_image_only\n",
        ")\n",
        "\n",
        "model_7, params_7, cv_acc_7, test_acc_7 = train_and_evaluate_model(\n",
        "    X_train_7, X_test_7, y_train_7, y_test_7, \"Run 7: Images Only\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SUMMARY OF ALL RUNS\n",
            "======================================================================\n",
            "\n",
            "                              CV Accuracy  Test Accuracy\n",
            "Run 1: First Order Only          0.882890       0.875380\n",
            "Run 2: Second Order Only         0.888213       0.878419\n",
            "Run 3: Both Orders               0.886692       0.875380\n",
            "Run 4: First Order + Images      0.881369       0.881459\n",
            "Run 5: Second Order + Images     0.881369       0.881459\n",
            "Run 6: Both Orders + Images      0.881369       0.881459\n",
            "Run 7: Images Only               0.881369       0.881459\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Summary of All Runs\n",
        "# ============================================================================\n",
        "print(\"=\" * 70)\n",
        "print(\"SUMMARY OF ALL RUNS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results = {\n",
        "    \"Run 1: First Order Only\": {\"CV Accuracy\": cv_acc_1, \"Test Accuracy\": test_acc_1},\n",
        "    \"Run 2: Second Order Only\": {\"CV Accuracy\": cv_acc_2, \"Test Accuracy\": test_acc_2},\n",
        "    \"Run 3: Both Orders\": {\"CV Accuracy\": cv_acc_3, \"Test Accuracy\": test_acc_3},\n",
        "    \"Run 4: First Order + Images\": {\"CV Accuracy\": cv_acc_4, \"Test Accuracy\": test_acc_4},\n",
        "    \"Run 5: Second Order + Images\": {\"CV Accuracy\": cv_acc_5, \"Test Accuracy\": test_acc_5},\n",
        "    \"Run 6: Both Orders + Images\": {\"CV Accuracy\": cv_acc_6, \"Test Accuracy\": test_acc_6},\n",
        "    \"Run 7: Images Only\": {\"CV Accuracy\": cv_acc_7, \"Test Accuracy\": test_acc_7},\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(results).T\n",
        "print(\"\\n\" + summary_df.to_string())\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
